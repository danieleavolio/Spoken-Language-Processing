{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c2RcCB-c_OVN",
        "olS8JwgmWKM6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spoken Language Processing 2023-24"
      ],
      "metadata": {
        "id": "f-u5GPNY94Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab3 - Dialogue Systems\n",
        "\n",
        "_Bruno Martins_\n",
        "\n",
        "\n",
        "This lab assignment will introduce tools and concepts related to the development of dialogue systems, exemplifying also the use of automatic speech recognition and text-to-speech models in this particular context.\n",
        "\n",
        "Students will be tasked with the development of a simple (spoken/conversational) question answering system, reusing different models associated to the HuggingFace Transformers library:\n",
        "\n",
        "* Speech recognition models (e.g., OpenAI Whisper).\n",
        "* Large language models for natural language understanding and generation (e.g., [LaMini-GPT2](https://huggingface.co/MBZUAI/LaMini-GPT-124M) models).\n",
        "* Text-to-speech models (e.g., SpeechT5).\n",
        "\n",
        "The first parts of this notebook will guide students in the use of the tools, while the last part presents the main problem that is to be tackled. Note that the first parts also feature intermediate tasks, which students are required to solve.\n",
        "\n",
        "To complete the project, student groups must deliver in Fenix an updated version of this notebook, featuring the proposed solutions to each task, together with a small PDF report (2 pages) outlining the methods that were developed (you can use the [following Overleaf template](https://www.overleaf.com/latex/templates/interspeech-2023-paper-kit/kzcdqdmkqvbr) for the report). The report can contain a section for each of the parts in the notebook, and the set of two documents should be uploaded in Fenix through a .zip file named after the number of the group.\n",
        "\n",
        "Students are encouraged to modify examples, incorporate any other techniques, and in general explore any approach that may permit improving the results. Assessment will be based on task completion, creativity in the proposed solutions, and overall accuracy over a benchmark dataset."
      ],
      "metadata": {
        "id": "pi2YtsXg96yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group identification\n",
        "\n",
        "Initialize the variable `group_id` with the number that Fenix assigned to your group and `student1_name`, `student1_id`, `student2_name` and `student2_id` with your names and student numbers."
      ],
      "metadata": {
        "id": "c2RcCB-c_OVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "print(f\"Group number: {group_id}\")\n",
        "print(f\"Student 1: {student1_name} ({student1_id})\")\n",
        "print(f\"Student 2: {student2_name} ({student2_id})\")"
      ],
      "metadata": {
        "id": "iA0jYy5U-qYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(group_id, int) and isinstance(student1_id, int) and isinstance(student2_id, int)\n",
        "assert isinstance(student1_name, str) and isinstance(student2_name, str)\n",
        "assert (group_id > 0) and (group_id < 40)\n",
        "assert (student1_id > 60000) and (student1_id < 120000) and (student2_id > 60000) and (student2_id < 120000)"
      ],
      "metadata": {
        "id": "WyoZ4YFu-rP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import Python packages\n",
        "\n",
        "NumPy is a Python library that provides functions to process multidimensional arrays. The NumPy documentation is available [here](https://numpy.org/doc/1.24/).\n",
        "\n",
        "[Librosa](https://librosa.org/) is a Python package for analyzing and processing audio signals. It provides a wide range of tools for tasks such as loading and manipulating audio files, extracting features from audio signals, and visualizing and playing back audio data.\n",
        "\n",
        "IPython display is a module in the IPython interactive computing environment that provides a set of functions for displaying various types of media in the Jupyter notebook or other IPython-compatible environments. For example, you can use the display() function to display an object in a notebook cell (for example an audio object).\n",
        "\n",
        "Matplotlib is a popular Python library that allows users to create a wide range of visualizations using a simple and intuitive syntax.\n",
        "\n",
        "Huggingface transformers provides APIs and tools to easily download and train state-of-the-art pretrained models based on the Transformer architecture. The documentation is available [here](https://huggingface.co/docs/transformers/index) and, for more details, look at the official [HuggingFace course](https://huggingface.co/course/chapter1/1).\n",
        "\n",
        "The associated HuggingFace libraries named [datasets](https://huggingface.co/docs/datasets/index) and [evaluate](https://huggingface.co/docs/evaluate/index) respectivly suport the direct access to many well-known datasets and common evaluation metrics used in NLP and speech research."
      ],
      "metadata": {
        "id": "wm3Vi8Yv_eRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install sentencepiece\n",
        "!pip3 install xformers\n",
        "!pip3 install transformers\n",
        "!pip3 install datasets\n",
        "!pip3 install evaluate\n",
        "!pip3 install jiwer\n",
        "!pip3 install librosa"
      ],
      "metadata": {
        "id": "4EP-NzX8_mXL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import datasets\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "_B61OfEZ_nYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using OpenAI Whisper\n",
        "\n",
        "Whisper is a cutting-edge model for for Automatic Speech Recognition (ASR), developed by OpenAI using a massive dataset of 680,000 hours of multilingual and multitask supervised data collected from the internet, and made available through the HuggingFace Transformers library.\n",
        "\n",
        "The following example illustrates the use of the Whisper model to transcribe a small audio sample taken from the LibriSpeech dataset (which is available through the HuggingFace datasets library).\n",
        "\n",
        "More detailed information about Whisper, including information on how to fine-tune the model with task-specific data, is available on a [tutorial in the HuggingFace blog](https://huggingface.co/blog/fine-tune-whisper)."
      ],
      "metadata": {
        "id": "YF4MxxW0AnHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
        "\n",
        "audio = ds[0][\"audio\"][\"array\"]\n",
        "audio = librosa.resample(audio, orig_sr=16000, target_sr=16000) # Resample audio to 16kHz (not needed in the case of this dataset)\n",
        "\n",
        "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "input_features = inputs.input_features\n",
        "\n",
        "display(Audio(audio, rate=16000)) # You are able to hear the audio inputs\n",
        "\n",
        "generated_ids = model.generate(inputs=input_features)\n",
        "transcription = processor.batch_decode(generated_ids, max_length=250, skip_special_tokens=True)[0]\n",
        "\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "FGUw-PlpLRAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic Speech Recognition (ASR) models are frequently evaluated through the Word Error Rate ([WER](https://huggingface.co/spaces/evaluate-metric/wer)).\n",
        "\n",
        "The WER is derived from the Levenshtein distance, working at the word level and aligning the recognized word sequence with the reference (spoken) word sequence using dynamic string alignment. The metric can then be computed as:\n",
        "\n",
        "WER = (S + D + I) / N = (S + D + I) / (S + D + C),\n",
        "\n",
        "where S is the number of substitutions, D is the number of deletions, I is the number of insertions, C is the number of correct words, and N is the number of words in the reference (N=S+D+C). The WER value indicates the average number of errors per reference word. The lower the value, the better the performance of the ASR system, with a WER of 0 being a perfect score.\n",
        "\n",
        "The example below illustrates the computation of the WER for two paired examples of a generated sentence versus a reference sentence. The score produced as output is the average value accross the two examples."
      ],
      "metadata": {
        "id": "cUaSIT3DUbJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "wer = load(\"wer\")\n",
        "predictions = [\"this is the prediction\", \"there is an other sample\"]\n",
        "references = [\"this is the reference\", \"there is another one\"]\n",
        "wer_score = wer.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(wer_score)"
      ],
      "metadata": {
        "id": "gKM72REnUi8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediate tasks:\n",
        "\n",
        "* Collect two small audio samples with your own voice, together with a transcription of the spoken messages. The following [example shows how to record audio from your microphone within a Python notebook running on Google Colab](https://colab.research.google.com/gist/ricardodeazambuja/03ac98c31e87caf284f7b06286ebf7fd/microphone-to-numpy-array-from-your-browser-in-colab.ipynb#scrollTo=H4rxNhsEpr-c), but you can use any other method to collect the audio samples.\n",
        "* Use the Whisper speech recognition model to transcribe the two spoken messages that were collected.\n",
        "* Use the transcriptions to compute the word error rate.\n",
        "* Experiment with the use of different recognition models (e.g., larger Whisper models), and see if the error rate changes."
      ],
      "metadata": {
        "id": "olS8JwgmWKM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your solutions to the exercises"
      ],
      "metadata": {
        "id": "jphGZ98sabip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LLMs for conditional language generation\n",
        "\n",
        "OpenAI GPT-2 is a language model based on the Transformer decoder architecture, trained with large scale data collected from the Web using a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. Thus, GPT-2 can be used to address problems like question answering, modeling the task as language generation conditioned in the question (plus other relevant additional context).\n",
        "\n",
        "The following example illustrates the use of the GPT-2 through the Huggingface Transformers library. In this case, instead of using the model directly, we are using the model through the pipeline API, which facilitates the adaptation to the case of other LLMs. The pipeline() function can be used to connect a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer."
      ],
      "metadata": {
        "id": "I6UxMbMXKgs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "set_seed(42) # make results deterministic\n",
        "\n",
        "generator = pipeline(model='gpt2')\n",
        "generator(\"Who is the president of the United States? The answer is\", max_length=15, num_return_sequences=1)"
      ],
      "metadata": {
        "id": "dUePzME4L1gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediate tasks:\n",
        "\n",
        "* Adapt the example showing how to use GPT-2 to do question answering over the [TriviaQA dataset](http://nlp.cs.washington.edu/triviaqa/) (you can use a [version](https://huggingface.co/datasets/lucadiliello/triviaqa) of this dataset from a previous shared task, which is available from HuggingFace datasets).\n",
        "* Evaluate the results obtained with different models (e.g., relatively small models trained to follow instructions, for instance from the [LaMini-LM](https://huggingface.co/MBZUAI/LaMini-GPT-124M) family, or quantized versions of larger models such as [Meta Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B)) and/or different usage strategies (e.g., consider different prompting strategies, parameter efficient fine-tuning, etc.).\n",
        "* Compute the error over the first 1000 examples from the validation split from the TriviaQA dataset, using the [BLEU metric](https://huggingface.co/spaces/evaluate-metric/bleu) for comparing the generated answers against the ground truth.\n"
      ],
      "metadata": {
        "id": "7oq2CoseWw47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your solutions to the exercises"
      ],
      "metadata": {
        "id": "cFSTj_X5aeah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using SpeechT5 for converting text-to-speech\n",
        "\n",
        "Motivated by the success of T5 (Text-To-Text Transfer Transformer) in different natural language processing tasks, the unified-modal SpeechT5 framework explores encoder-decoder pre-training for self-supervised speech/text representation learning.\n",
        "\n",
        "The model is again conveniently available through the HuggingFace Transformers library. The following example illustrates the use of the SpeechT5 model for generating a spectrogram from a textual input, together with a neural vocoder model for producing a speech signal.\n",
        "\n",
        "More detailed information about SpeechT5 is available on a [tutorial on the HuggingFace blog](https://huggingface.co/blog/speecht5)."
      ],
      "metadata": {
        "id": "nWdpIcs_Kl9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, set_seed\n",
        "from IPython.display import Audio\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "set_seed(42) # make results deterministic\n",
        "\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "\n",
        "inputs = processor(text=\"Hello, my dog is cute.\", return_tensors=\"pt\")\n",
        "speaker_embeddings = torch.zeros((1, 512))\n",
        "\n",
        "# When using SpeechT5 for TTS, you should use \"xvector speaker embeddings\"\n",
        "# to customize the output to a particular speaker’s voice characteristics\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embeddings = torch.tensor(embeddings_dataset[42][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)\n",
        "with torch.no_grad(): speech = vocoder(spectrogram)\n",
        "\n",
        "display(Audio(speech.numpy(), rate=16000)) # You can hear the audio inputs\n",
        "\n",
        "# You can plot the generated spectrogram\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()\n",
        "plt.imshow(spectrogram.T)\n",
        "plt.show()\n",
        "\n",
        "librosa.display.waveshow(speech.numpy(), sr=16000) # You can plot the generated waveform\n",
        "\n",
        "sf.write(\"tts_example.wav\", speech.numpy(), samplerate=16000) # You can save the audio to a .wav file"
      ],
      "metadata": {
        "id": "x5MpgFRpPVzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediate tasks:\n",
        "\n",
        "* Connect the results from your answer to the previous intermediate task (i.e., conditioned language generation) to the SpeechT5 text-to-speech model, so as to produce speech outputs from the text generated by the model.\n",
        "* Produce speech-based answers for the first 5 questions in the validation split from the TriviaQA dataset.\n",
        "* Connect also the results from your answer to the first intermediate task (i.e., automated speech recognition) to the SpeechT5 model and the LLM, so as to take spoken questions as input and produce a speech output.\n",
        "* Take the audio samples from 10 TriviaQA questions (as available in connection to the [SLUE-SQA-5 dataset](https://huggingface.co/datasets/asapp/slue-phase-2), in Huggingface datasets), and evaluate the answers generated for the spoken questions using the BLEU metric.\n",
        "* Collect small audio samples, with your own voice, for the first 2 questions in the validation split from the TriviaQA dataset, and produce speech-based answers for these two questions.\n"
      ],
      "metadata": {
        "id": "gcPiO_TWZvZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your solutions to the exercises"
      ],
      "metadata": {
        "id": "-Lo5bzgxafO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main problem\n",
        "\n",
        "Students are tasked with joining together the speech recognition, language understanding and generation, and text-to-speech models, in order to build a conversational spoken question answering approach.\n",
        "\n",
        "* The method should take as input speech utterances with questions.\n",
        "* The language understanding and generation component should use as input a transcription for the current speech utterance, and optionally also transcriptions from previous speech utterances (i.e., the conversation context).\n",
        "* The language understanding and generation component can explore different strategies for improving answer quality:\n",
        "  * Use of large LLMs trained to follow instructions, e.g. with reinforcement learning from human feedback.\n",
        "  * Prompting the language model with retrieved in-context examples.\n",
        "  * Using parameter-efficient fine-ting with existing conversational question answering datasets (e.g., [the CoQA dataset](https://stanfordnlp.github.io/coqa/), which is [also available](https://huggingface.co/datasets/stanfordnlp/coqa) from HuggingFace datasets).\n",
        "  * ...\n",
        "* The text-to-speech component takes as input the results from language generation, and produces a speech output.\n",
        "* Both the automated speech recognition and the text-to-speech components can explore different approaches, although students should attempt to justify their choices (e.g., if changing the automated speech recognition component, show that it achieves a lower WER).\n",
        "* Collect small audio samples, with your own voice, for the questions in the first instance in the CoQA validation split, and show the results produced by your method for these questions."
      ],
      "metadata": {
        "id": "GqEUY96xQG3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your solutions to the exercises"
      ],
      "metadata": {
        "id": "cJZitDMkb7vl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}